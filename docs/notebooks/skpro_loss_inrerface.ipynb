{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SkPro loss function Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SkPro proposes an object oriented loss function class implementation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title IMPORTS\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from skpro.workflow.manager import DataManager\n",
    "from skpro.distributions.distribution_base import Mode\n",
    "from skpro.baselines.classical_baselines import ClassicalBaseline\n",
    "from skpro.metrics.classical_loss import SquaredError\n",
    "\n",
    "data = DataManager('boston')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classical Losses\n",
    "\n",
    "The 'classical losses' map a vector of __scalar predictions__ and a vector of targets onto the real. They all inherit from a common 'LossFunction' base abstract class and implements a '__call__' method (i.e. () operator override) that returns the vector of losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting: classical\n",
      "total squared error: 33.44897999767653\n"
     ]
    }
   ],
   "source": [
    "baseline =  ClassicalBaseline(LinearRegression()).fit(data.X_train, data.y_train)\n",
    "estimator = LinearRegression().fit(data.X_train, data.y_train)\n",
    "\n",
    "loss_func = SquaredError()\n",
    "losses = loss_func(estimator.predict(data.X_test), data.y_test)\n",
    "\n",
    "print('setting: ' + str(loss_func.type()))\n",
    "print('total squared error: ' + str(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probabilisitc Losses\n",
    "\n",
    "The 'probabilistic losses' map a vector of __distribution predictions__ (i.e. skpro distribution object) and a vector of targets onto the real. They all inherit from a common 'LossFunction' base abstract class and implements a '__call__' method (i.e. () operator override) that returns the vector of losses.\n",
    "\n",
    "PS : The distribution mode must eventually be set to 'ELEMENT_WISE' to output a loss vector in a 'one a one for one basis. ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting: probabilistic\n",
      "total squared error: 3.3375354432750513\n"
     ]
    }
   ],
   "source": [
    "from skpro.baselines.classical_baselines import ClassicalBaseline\n",
    "\n",
    "from skpro.metrics.proba_loss_cont import LogLossClipped\n",
    "from skpro.metrics.proba_scorer import ProbabilisticScorer\n",
    "\n",
    "baseline =  ClassicalBaseline(LinearRegression()).fit(data.X_train, data.y_train)\n",
    "dist = baseline.predict_proba(data.X_test) \n",
    "dist.setMode(Mode.ELEMENT_WISE)\n",
    "\n",
    "loss_func = LogLossClipped(cap = np.exp(-23))\n",
    "losses = loss_func(dist, data.y_test)\n",
    "\n",
    "print('setting: ' + str(loss_func.type()))\n",
    "print('total squared error: ' + str(np.mean(losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 'Scorer' can be used to symplify the procedure above. It acts as a wrap-up class that directly evaluates an error given a loss functor and a probabilistic estimator. The loss function is passed in the Scorer constructor. \n",
    "\n",
    "A '__call__' implementation returns the score. It takes as argument : a probabilistic estimator, an array of test samples (X), an array of targets (y) and a string specifying the format of the output (mode : taking values in ['average', 'absolute'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total log-loss error from scorer: 3.3375354432750513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scorer = ProbabilisticScorer(LogLossClipped(cap = np.exp(-23)))\n",
    "score = scorer(estimator = baseline, X = data.X_test, y = data.y_test, mode = 'average')\n",
    "print('total log-loss error from scorer: ' + str(np.mean(losses)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
